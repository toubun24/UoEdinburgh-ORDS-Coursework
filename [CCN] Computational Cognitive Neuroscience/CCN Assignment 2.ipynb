{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe4319c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import copy\n",
    "from scipy.optimize import minimize \n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "sns.set()\n",
    "mpl.use('pgf') # for latex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b22b0705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 45.42\n",
      "Standard deviation: 15.92\n",
      "Median: 42.5\n",
      "Count of healthy controls (STAI≤43): 25\n",
      "Indices of the control subjects: [21, 25, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "Count of unfitted healthy controls: 1\n",
      "Count of choosing A: [29, 40, 55, 15, 46, 36, 55, 39, 30, 33, 44, 27, 27, 32, 43, 39, 15, 23, 48, 48, 28, 41, 39, 24, 61, 33, 48, 47, 38, 22, 66, 40, 34, 46, 35, 51, 37, 33, 65, 50, 29, 26, 34, 55, 23, 58, 27, 28, 41, 22]\n",
      "Average percentage of choosing A (subject 1): 0.18\n",
      "Average count of choosing A: 0.24\n",
      "Random strategy expected count of aversive sound: 106\n"
     ]
    }
   ],
   "source": [
    "# (a): Exploring the data (6)\n",
    "\n",
    "# read .csv\n",
    "score = pd.read_csv(\"stai_scores.csv\", header = None)[0]\n",
    "choices = pd.read_csv(\"inst_choices.csv\", header = None)\n",
    "outcomes = pd.read_csv(\"inst_outcomes.csv\", header = None)\n",
    "\n",
    "# statistics\n",
    "score_mean = np.mean(score)\n",
    "score_std = np.std(score)\n",
    "score_median = np.median(score)\n",
    "\n",
    "# cutoff count\n",
    "cutoff_count = sum(score <= 43)\n",
    "cutoff_index = [i for i in range(len(score)) if score[i] <= 43]\n",
    "cutoff_count_unfit = sum(score[25:50] > 43)\n",
    "\n",
    "# choices & outcomes count\n",
    "choices_count = []\n",
    "for i in range(50):\n",
    "    choices_count.append(sum(choices.loc[i] == 1))\n",
    "choices_count_sub1 = choices_count[0] / 160\n",
    "average_choices_percentage = np.mean(choices_count) / 160\n",
    "random_aversive_sound = 160 * 0.25 * (0.6 + 0.8 + 0.6 + 0.65)\n",
    "\n",
    "# output\n",
    "print(\"Mean:\", score_mean)\n",
    "print(\"Standard deviation:\", round(score_std, 2))\n",
    "print(\"Median:\", score_median)\n",
    "print(\"Count of healthy controls (STAI≤43):\", cutoff_count)\n",
    "print(\"Indices of the control subjects:\", cutoff_index)\n",
    "print(\"Count of unfitted healthy controls:\", cutoff_count_unfit)\n",
    "print(\"Count of choosing A:\", choices_count)\n",
    "print(\"Average percentage of choosing A (subject 1):\", round(choices_count_sub1, 2))\n",
    "print(\"Average count of choosing A:\", round(average_choices_percentage, 2))\n",
    "print(\"Random strategy expected count of aversive sound:\", round(random_aversive_sound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "705d1280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of aversive sounds: 60.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-27bd0dce688c>:74: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# (b): Simulations (7)\n",
    "\n",
    "# update strategy 1\n",
    "def update_1(V, action, outcome, alpha):\n",
    "    V[action-1] = V[action-1] + float(alpha) * (outcome - V[action-1])\n",
    "    return V\n",
    "\n",
    "# softmax decision\n",
    "def softmax_decision(V, beta):\n",
    "    p_action_A = 1 / (1 + np.exp(-beta * (V[1] - V[0])))\n",
    "    return p_action_A\n",
    "\n",
    "# simulation\n",
    "def simulation(V0, n, update, decision, alpha, beta, sample = False):\n",
    "    p_outcome_A1 = [0.6, 0.8, 0.6, 0.65]\n",
    "    V_AB = np.zeros([161, 2])\n",
    "    aver_stim = 0\n",
    "    if sample == True:\n",
    "        outcomes_sample = np.zeros(160)\n",
    "        choices_sample = np.zeros(160)\n",
    "    for s in range(n):\n",
    "        V = copy.deepcopy(V0)\n",
    "        V_AB[0,:] = V_AB[0,:] + np.array(V)\n",
    "        for k in range(4):\n",
    "            for i in range(40):\n",
    "                # action\n",
    "                if decision(V, beta) >= np.random.rand(1):\n",
    "                    action = 1\n",
    "                    p_outcome_1 = p_outcome_A1[k]\n",
    "                else:\n",
    "                    action = 2\n",
    "                    p_outcome_1 = 1 - p_outcome_A1[k]\n",
    "                # outcome\n",
    "                if p_outcome_1 >= np.random.rand(1):\n",
    "                    outcome = 1\n",
    "                    aver_stim = aver_stim + 1\n",
    "                else:\n",
    "                    outcome = 0\n",
    "                # update\n",
    "                V = update(V, action, outcome, alpha)\n",
    "                V_AB[k*40+i+1, :] = V_AB[k*40+i+1, :] + np.array(V)\n",
    "                # record\n",
    "                if sample == True:\n",
    "                    choices_sample[k*40+i] = int(action)\n",
    "                    outcomes_sample[k*40+i] = int(outcome)\n",
    "    V_AB = V_AB / n\n",
    "    aver_stim = aver_stim / n\n",
    "    if sample == True:\n",
    "        return choices_sample, outcomes_sample\n",
    "    return V_AB, aver_stim\n",
    "\n",
    "# config\n",
    "V0 = [0.5, 0.5]\n",
    "n = 10000\n",
    "update = update_1\n",
    "decision = softmax_decision\n",
    "alpha = 0.3\n",
    "beta = 8\n",
    "\n",
    "# implement\n",
    "V_AB_1, aver_stim_1 = simulation(V0, n, update, decision, alpha, beta)\n",
    "\n",
    "# plot\n",
    "fig, ax =  plt.subplots(1, 1, sharex = True, figsize = (6,3.7))\n",
    "label = ['V(A)', 'V(B)', 'V(A)-V(B)']\n",
    "color = ['#FFC75F', '#FF9671', '#FF6F91']\n",
    "ax.plot(np.arange(0, 161, 1), V_AB_1[:, 0], label = label[0], color = color[0])\n",
    "ax.plot(np.arange(0, 161, 1), V_AB_1[:, 1], label = label[1], color = color[1])\n",
    "ax.plot(np.arange(0, 161, 1), V_AB_1[:, 0] - V_AB_1[:,1], label = label[2], \\\n",
    "        color = color[2])\n",
    "ax.set_title('Simulation evolution (n=10000)')\n",
    "ax.set_xlabel('Trials')\n",
    "ax.set_ylabel('Average action values')\n",
    "ax.legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n",
    "# check\n",
    "print(\"Average number of aversive sounds:\", aver_stim_1)\n",
    "\n",
    "# image export\n",
    "# plt.savefig('1.pgf', format='pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62e8afa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-23ee5fc32610>:37: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# (c): Exploring parameter settings (7)\n",
    "\n",
    "# config\n",
    "V0 = [0.5, 0.5]\n",
    "n = 2\n",
    "update = update_1\n",
    "decision = softmax_decision\n",
    "alpha = 0\n",
    "beta0 = 0\n",
    "intv = 0.05\n",
    "sepn = int(1/intv) - 1\n",
    "\n",
    "# implement\n",
    "aver_stim = np.zeros([sepn,sepn])\n",
    "for i in range(sepn):\n",
    "    alpha = alpha + intv\n",
    "    beta = beta0\n",
    "    for j in range(sepn):\n",
    "        beta = beta + 10 * intv\n",
    "        __, aver_stim_ab = simulation(V0, n, update, decision, alpha, beta)\n",
    "        aver_stim[i,j] = aver_stim_ab\n",
    "\n",
    "# plot\n",
    "fig = plt.figure(figsize = (6,4.7))\n",
    "ax = fig.gca(projection = '3d')\n",
    "aa = np.arange(intv, 1, intv)\n",
    "bb = np.arange(10*intv, 10, 10*intv)\n",
    "beta_axis, alpha_axis = np.meshgrid(bb, aa)\n",
    "alpha_axis.resize(sepn*sepn)\n",
    "beta_axis.resize(sepn*sepn)\n",
    "aver_stim.resize(sepn*sepn)\n",
    "plot_c = ax.plot_trisurf(beta_axis, alpha_axis, aver_stim, cmap = plt.cm.viridis, \\\n",
    "                         linewidth = 0)\n",
    "fig.colorbar(plot_c)\n",
    "ax.set_title('Average number of aversive stimuli (n=1000)')\n",
    "ax.set_xlabel(r'$\\beta$')\n",
    "ax.set_ylabel(r'$\\alpha$')\n",
    "plt.show()\n",
    "\n",
    "# image export\n",
    "# plt.savefig('2.pgf', format='pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d104693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLL for 1st participant is: 50.19\n",
      "NLL for 2nd participant is: 64.24\n",
      "NLL for 10th participant is: 58.6\n"
     ]
    }
   ],
   "source": [
    "# (d): Likelihood function (6)\n",
    "\n",
    "# negative log likelihood\n",
    "def nll(theta, choices, outcomes, V0):\n",
    "    length_num = len(choices)\n",
    "    V = copy.deepcopy(V0)\n",
    "    nll = np.zeros(length_num)\n",
    "    for i in range(length_num):\n",
    "        c = int(choices[i])\n",
    "        c_alt = 1 + int(c==1)\n",
    "        nll[i] = 1 / (1 + np.exp(-theta[1] * (V[c_alt-1] - V[c-1])))\n",
    "        V[c-1] = V[c-1] + theta[0] * (int(outcomes[i]) - V[c-1])\n",
    "    nll = -np.sum(np.log(nll))\n",
    "    return nll\n",
    "\n",
    "# config\n",
    "V0 = [0.5, 0.5]\n",
    "theta = [0.3, 8]\n",
    "\n",
    "# implement\n",
    "nll_1 = nll(theta, choices.loc[0], outcomes.loc[0], V0)\n",
    "nll_2 = nll(theta, choices.loc[1], outcomes.loc[1], V0)\n",
    "nll_10 = nll(theta, choices.loc[9], outcomes.loc[9], V0)\n",
    "\n",
    "# output\n",
    "print(\"NLL for 1st participant is:\", round(nll_1,2))\n",
    "print(\"NLL for 2nd participant is:\", round(nll_2,2))\n",
    "print(\"NLL for 10th participant is:\", round(nll_10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2341cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of fitted parameter values [alpha, beta]: [0.42, 5.28]\n",
      "Variance of fitted parameter values [alpha, beta]: [0.01, 3.04]\n",
      "Pearson’s correlation coefficient (total): 0.07\n",
      "Pearson’s correlation coefficient (group 1): -0.13\n",
      "Pearson’s correlation coefficient (group 2): -0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-f11059d986d6>:39: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# (e): Model fitting (9)\n",
    "\n",
    "# optimization\n",
    "V0 = [0.5, 0.5]\n",
    "theta_opt = np.zeros([50,2])\n",
    "nll_a_obj = np.zeros(50)\n",
    "for i in range(50):    \n",
    "    nll_result = minimize(nll, [0.3, 8], args = (choices.loc[i], outcomes.loc[i], \\\n",
    "                                                 V0), method = \"Nelder-Mead\")\n",
    "    theta_opt[i, :] = nll_result['x']\n",
    "    nll_a_obj[i] = nll_result['fun']\n",
    "\n",
    "# statistics\n",
    "theta_mean = np.mean(theta_opt, axis = 0)\n",
    "theta_var = np.var(theta_opt, axis = 0)\n",
    "theta_cor = np.corrcoef(theta_opt[:, 0], theta_opt[:, 1])[0, 1]\n",
    "theta_cor_1 = np.corrcoef(theta_opt[0:25, 0], theta_opt[0:25, 1])[0, 1]\n",
    "theta_cor_2 = np.corrcoef(theta_opt[25:50, 0], theta_opt[25:50, 1])[0, 1]\n",
    "\n",
    "# plot\n",
    "fig, ax =  plt.subplots(1, 1, sharex = True, figsize = (6,3.7))\n",
    "label = [r'$\\alpha$', r'0.1$\\beta$']\n",
    "color = ['#FF9671', '#FF6F91']\n",
    "# color = ['#FF6F91', '#D65DB1']\n",
    "for i in range(50):\n",
    "    plt.vlines(x = i+1, ymin = theta_opt[i, 0], ymax = 0.1*theta_opt[i, 1], \\\n",
    "               color = '#FFC75F', zorder = 1)\n",
    "plt.axvline(x = 25.5, ymin = 0, ymax = 1, linestyle = \"dashed\", \\\n",
    "            color = '#FFC75F', zorder = 2)\n",
    "plt.axhline(y = np.mean(theta_opt[:, 0]), xmin = 0, xmax = 1, \\\n",
    "            linestyle = \"dashed\", color = color[0], zorder = 2)\n",
    "plt.axhline(y = 0.1*np.mean(theta_opt[:, 1]), xmin = 0, xmax = 1, \\\n",
    "            linestyle = \"dashed\", color = color[1], zorder = 2)\n",
    "plt.axhline(y = np.mean(theta_opt[0:25, 0]), xmin = 0, xmax = 0.5, \\\n",
    "            color = color[0], zorder = 2)\n",
    "plt.axhline(y = 0.1*np.mean(theta_opt[0:25, 1]), xmin = 0, xmax = 0.5, \\\n",
    "            color = color[1], zorder = 2)\n",
    "plt.axhline(y = np.mean(theta_opt[25:50, 0]), xmin = 0.5, xmax = 1, \\\n",
    "            color = color[0], zorder = 2)\n",
    "plt.axhline(y = 0.1*np.mean(theta_opt[25:50, 1]), xmin = 0.5, xmax = 1, \\\n",
    "            color = color[1], zorder = 2)\n",
    "ax.scatter(np.arange(1, 51, 1), theta_opt[:, 0], label = label[0], \\\n",
    "           color = color[0], zorder = 3)\n",
    "ax.scatter(np.arange(1, 51, 1), 0.1*theta_opt[:, 1], label = label[1], \\\n",
    "           color = color[1], zorder = 3)\n",
    "ax.set_title('Parameter optimization of NLL by Nelder-Mead')\n",
    "ax.set_xlabel('Participant index')\n",
    "ax.set_ylabel('Parameter value')\n",
    "ax.legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n",
    "# output\n",
    "print(\"Mean of fitted parameter values [alpha, beta]:\", \\\n",
    "      [round(i,2) for i in theta_mean])\n",
    "print(\"Variance of fitted parameter values [alpha, beta]:\", \\\n",
    "      [round(i,2) for i in theta_var])\n",
    "print(\"Pearson’s correlation coefficient (total):\", round(theta_cor,2))\n",
    "print(\"Pearson’s correlation coefficient (group 1):\", round(theta_cor_1,2))\n",
    "print(\"Pearson’s correlation coefficient (group 2):\", round(theta_cor_2,2))\n",
    "\n",
    "# image export\n",
    "# plt.savefig('3.pgf', format='pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac183384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of alpha in 2 groups: 0.47 0.38\n",
      "The t-statistic of alpha: 3.03\n",
      "The p-value of alpha: 0.0\n",
      "The mean of beta in 2 groups: 6.03 4.53\n",
      "The t-statistic of beta: 3.3\n",
      "The p-value of beta: 0.0\n",
      "Degrees of freedom: 48\n"
     ]
    }
   ],
   "source": [
    "# (f): Group comparison (4)\n",
    "\n",
    "# t-test\n",
    "alpha_mean_1 = theta_opt[0:25, 0]\n",
    "alpha_mean_2 = theta_opt[25:50, 0]\n",
    "beta_mean_1 = theta_opt[0:25, 1]\n",
    "beta_mean_2 = theta_opt[25:50, 1]\n",
    "t_alpha = stats.ttest_ind(alpha_mean_1, alpha_mean_2)\n",
    "t_beta = stats.ttest_ind(beta_mean_1, beta_mean_2)\n",
    "\n",
    "# output\n",
    "print(\"The mean of alpha in 2 groups:\", round(np.mean(alpha_mean_1), 2), \\\n",
    "      round(np.mean(alpha_mean_2), 2))\n",
    "print(\"The t-statistic of alpha:\", round(t_alpha[0], 2))\n",
    "print(\"The p-value of alpha:\", round(t_alpha[1], 2))\n",
    "print(\"The mean of beta in 2 groups:\", round(np.mean(beta_mean_1), 2), \\\n",
    "      round(np.mean(beta_mean_2), 2))\n",
    "print(\"The t-statistic of beta:\", round(t_beta[0], 2))\n",
    "print(\"The p-value of beta:\", round(t_beta[1], 2))\n",
    "print(\"Degrees of freedom:\", 48) # (n1+n2-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e1795af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8af80401329a>:11: RuntimeWarning: overflow encountered in exp\n",
      "  nll[i] = 1 / (1 + np.exp(-theta[1] * (V[c_alt-1] - V[c-1])))\n",
      "<ipython-input-5-8af80401329a>:13: RuntimeWarning: divide by zero encountered in log\n",
      "  nll = -np.sum(np.log(nll))\n",
      "<ipython-input-9-e55ae3419970>:123: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n",
      "<ipython-input-9-e55ae3419970>:144: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson's correlation of alpha: [0.25, 0.38, 0.4, 0.57, 0.43]\n",
      "Pearson's correlation of beta: [0.7, 0.68, 0.65, 0.73, 0.5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-e55ae3419970>:157: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# (g): Parameter recovery (9)\n",
    "\n",
    "# config\n",
    "mu_alpha = [np.mean(alpha_mean_1), np.mean(alpha_mean_2)]\n",
    "mu_beta = [np.mean(beta_mean_1), np.mean(beta_mean_2)]\n",
    "sigma_alpha = 0.01\n",
    "sigma_beta = 0.5\n",
    "V0 = [0.5, 0.5]\n",
    "n = 1\n",
    "update = update_1\n",
    "decision = softmax_decision\n",
    "repeat_num = 5\n",
    "participants_num = np.arange(4, 54, 5)\n",
    "trials_num = np.arange(5, 165, 5)\n",
    "\n",
    "# recovery\n",
    "theta_cor_samples = np.zeros([repeat_num, 2])\n",
    "choices_sample = np.zeros([50, 160])\n",
    "outcomes_sample = np.zeros([50, 160])\n",
    "theta_samples_fit = np.zeros([50, 2])\n",
    "for r in range(repeat_num):\n",
    "    # sample parameters\n",
    "    theta_sample = np.zeros([50, 2])\n",
    "    theta_sample_record = np.zeros([50, 2, 20])\n",
    "    for k in range(2):\n",
    "        for i in range(25):\n",
    "            # ensure alpha and beta in mean+-1.96*std\n",
    "            j_a = 0\n",
    "            j_b = 0\n",
    "            while theta_sample[k*25+i, 0] <= mu_alpha[k] - 1.96 * sigma_alpha \\\n",
    "            or theta_sample[k*25+i, 0] >= mu_alpha[k] + 1.96 * sigma_alpha:\n",
    "                theta_sample[k*25+i, 0] = np.random.normal(mu_alpha[k], sigma_alpha)\n",
    "                theta_sample_record[k*25+i, 0, j_a] = theta_sample[k*25+i, 0]\n",
    "                j_a = j_a + 1\n",
    "            while theta_sample[k*25+i, 1] <= mu_beta[k] - 1.96 * sigma_beta \\\n",
    "            or theta_sample[k*25+i, 1] >= mu_beta[k] + 1.96 * sigma_beta:\n",
    "                theta_sample[k*25+i, 1] = np.random.normal(mu_beta[k], sigma_beta)\n",
    "                theta_sample_record[k*25+i, 1, j_b] = theta_sample[k*25+i, 1]\n",
    "                j_b = j_b + 1\n",
    "    # simulation and fit by optimizing NLL and statistics\n",
    "    for i in range(50):\n",
    "        choices_sample[i, :], outcomes_sample[i, :] = \\\n",
    "        simulation(V0, n, update, decision, theta_sample[i, 0], \\\n",
    "                   theta_sample[i, 1], sample = True)\n",
    "        theta_samples_fit[i, :] = \\\n",
    "        minimize(nll, [0.3, 8], args = (choices_sample[i, :], \\\n",
    "                                        outcomes_sample[i, :], V0), \\\n",
    "                 method = \"Nelder-Mead\")['x']\n",
    "    theta_cor_samples[r, 0] = np.corrcoef(theta_samples_fit[:, 0], \\\n",
    "                                          theta_sample[:, 0])[0, 1]\n",
    "    theta_cor_samples[r, 1] = np.corrcoef(theta_samples_fit[:, 1], \\\n",
    "                                          theta_sample[:, 1])[0, 1]\n",
    "\n",
    "# explore\n",
    "choices_sample = np.zeros([50, 160])\n",
    "outcomes_sample = np.zeros([50, 160])\n",
    "theta_cor_sample = np.zeros([len(participants_num), len(trials_num), 2])\n",
    "theta_sample_fit = np.zeros([50, len(trials_num), 2])\n",
    "theta_sample_copy = np.zeros([50, 2])\n",
    "for i in range(25):\n",
    "    theta_sample_copy[2*i, :] = theta_sample[i, :]\n",
    "    theta_sample_copy[2*i+1, :] = theta_sample[25+i, :]\n",
    "for i in range(50):\n",
    "    if (i in participants_num) is True:\n",
    "        i_index = participants_num.tolist().index(i)\n",
    "    choices_sample[i, :], outcomes_sample[i, :] = \\\n",
    "    simulation(V0, n, update, decision, theta_sample_copy[i, 0], \\\n",
    "               theta_sample_copy[i, 1], sample = True)\n",
    "    for j in range(len(trials_num)):\n",
    "        theta_sample_fit[i, j, :] = \\\n",
    "        minimize(nll, [0.3, 8], args = (choices_sample[i, 0:trials_num[j]], \\\n",
    "                                        outcomes_sample[i, 0:trials_num[j]], V0), \\\n",
    "                 method = \"Nelder-Mead\")['x']\n",
    "        if (i in participants_num) is True:\n",
    "            theta_cor_sample[i_index, j, 0] = \\\n",
    "            np.corrcoef(theta_sample_fit[0:(i+1), j, 0], \\\n",
    "                        theta_sample_copy[0:(i+1), 0])[0, 1]\n",
    "            theta_cor_sample[i_index, j, 1] = \\\n",
    "            np.corrcoef(theta_sample_fit[0:(i+1), j, 1], \\\n",
    "                        theta_sample_copy[0:(i+1), 1])[0, 1]\n",
    "\n",
    "# plot\n",
    "# fig, ax =  plt.subplots(1, 1, sharex = True, figsize = (6,3.7))\n",
    "# label = [r'$Cor(\\alpha)$', r'$Cor(\\beta)$']\n",
    "# color = ['#FF9671', '#FF6F91']\n",
    "# for i in range(5):\n",
    "#     plt.vlines(x = i+1, ymin = theta_cor_sample[i, -1, -1, 0], \\\n",
    "# ymax = theta_cor_sample[i, -1, -1, 1], color = '#FFC75F', zorder = 1)\n",
    "# plt.axhline(y = np.mean(theta_cor_sample[:, -1, -1, 0]), xmin = 0, xmax = 6, \\\n",
    "# linestyle = \"dashed\", color = color[0], zorder = 2)\n",
    "# plt.axhline(y = np.mean(theta_cor_sample[:, -1, -1, 1]), xmin = 0, xmax = 6, \\\n",
    "# linestyle = \"dashed\", color = color[1], zorder = 2)\n",
    "# ax.scatter(np.arange(1, repeat_num+1, 1), theta_cor_sample[:, -1, -1, 0], \\\n",
    "# label = label[0], color = color[0], zorder = 3)\n",
    "# ax.scatter(np.arange(1, repeat_num+1, 1), theta_cor_sample[:, -1, -1, 1], \\\n",
    "# label = label[1], color = color[1], zorder = 3)\n",
    "# ax.set_title('Pearson’s correlation between simulated and re-fitted parameters')\n",
    "# ax.set_xlabel('Trials')\n",
    "# ax.set_ylabel('Pearson’s correlation')\n",
    "# ax.legend(loc = 'upper right')\n",
    "# plt.show()\n",
    "\n",
    "# plot\n",
    "fig, ax =  plt.subplots(1, 2, sharey = True, figsize = (8,3.7))\n",
    "label = [r'$\\alpha$', r'$0.1\\beta$']\n",
    "color = ['#FF9671', '#FF6F91']\n",
    "#for i in range(5):\n",
    "#    ax[0].vlines(x = i+1, ymin = theta_cor_sample[i, -1, -1, 0], ymax = \\\n",
    "# theta_cor_sample[i, -1, -1, 1], color = '#FFC75F', zorder = 1)\n",
    "for i in range(2):\n",
    "    ax[0].axhline(y = mu_alpha[i], xmin = 0.5*i, xmax = 0.5*(i+1), \\\n",
    "                  color = color[0], zorder = 2)\n",
    "    ax[0].axhline(y = 0.1*mu_beta[i], xmin = 0.5*i, xmax = 0.5*(i+1), \\\n",
    "                  color = color[1], zorder = 2)\n",
    "ax[0].axhline(y = np.mean(mu_alpha[:]), xmin = 0, xmax = 1, \\\n",
    "              linestyle = \"dashed\", color = color[0], zorder = 2)\n",
    "ax[0].axhline(y = 0.1*np.mean(mu_beta[:]), xmin = 0, xmax = 1, \\\n",
    "              linestyle = \"dashed\", color = color[1], zorder = 2)\n",
    "ax[0].scatter(np.arange(1, 51, 1), theta_sample[:, 0], label = label[0], \\\n",
    "              color = color[0], zorder = 3)\n",
    "ax[0].scatter(np.arange(1, 51, 1), 0.1 * theta_sample[:, 1], label = label[1], \\\n",
    "              color = color[1], zorder = 3)\n",
    "ax[0].axvline(x = 25.5, ymin = 0, ymax = 1, linestyle = \"dashed\", \\\n",
    "              color = '#FFC75F', zorder = 2)\n",
    "for i in range (25):\n",
    "    for k in range(2):\n",
    "        for j in range(20):\n",
    "            if (theta_sample_record[25*k+i, 0, j] <= mu_alpha[k] - 1.96 * \\\n",
    "                sigma_alpha or theta_sample_record[25*k+i, 0, j] >= mu_alpha[k] + \\\n",
    "                1.96 * sigma_alpha) and theta_sample_record[25*k+i, 0, j] != 0:\n",
    "                ax[0].scatter(25*k+i+1, theta_sample_record[25*k+i, 0, j], \\\n",
    "                              color = '#D65DB1', zorder = 3)\n",
    "            if (theta_sample_record[25*k+i, 1, j] <= mu_beta[k] - 1.96 * sigma_beta \\\n",
    "                or theta_sample_record[25*k+i, 1, j] >= mu_beta[k] + 1.96 * sigma_beta) \\\n",
    "            and theta_sample_record[25*k+i, 1, j] != 0:\n",
    "                ax[0].scatter(25*k+i+1, 0.1*theta_sample_record[25*k+i, 1, j], \\\n",
    "                              color = '#845EC2', zorder = 3)\n",
    "ax[0].set_title('Simulated params and outliers')\n",
    "ax[0].set_xlabel('Participant index')\n",
    "ax[0].set_ylabel('Simulated parameters')\n",
    "ax[0].legend(loc = 'upper right')\n",
    "ax[1].scatter(theta_samples_fit[:, 0], theta_sample[:, 0], label = label[0], \\\n",
    "              color = color[0], zorder = 3)\n",
    "ax[1].scatter(0.1*theta_samples_fit[:, 1], 0.1*theta_sample[:, 1], label = label[1], \\\n",
    "              color = color[1], zorder = 3)\n",
    "alpha_min = np.min([np.min(theta_sample[:, 0]), np.min(theta_samples_fit[:, 0])])\n",
    "beta_min = 0.1 * np.min([np.min(theta_sample[:, 1]), np.min(theta_samples_fit[:, 1])])\n",
    "alpha_max = np.max([np.max(theta_sample[:, 0]), np.max(theta_samples_fit[:, 0])])\n",
    "beta_max = 0.1 * np.max([np.max(theta_sample[:, 1]), np.max(theta_samples_fit[:, 1])])\n",
    "lim_min = np.min([alpha_min, beta_min])\n",
    "lim_max = np.max([alpha_max, beta_max])\n",
    "lim_min = np.min([lim_min, 0.1*np.min(theta_sample_record[theta_sample_record>0])]) - 0.05\n",
    "lim_max = np.max([lim_max, 0.1*np.max(theta_sample_record[theta_sample_record>0])]) + 0.05\n",
    "x_grid = np.arange(lim_min, lim_max, 0.01)\n",
    "ax[1].set_ylim(lim_min, lim_max)\n",
    "ax[1].set_xlim(lim_min, lim_max)\n",
    "ax[1].plot(x_grid, x_grid, linestyle = \"dashed\", color = '#FFC75F', zorder = 2)\n",
    "ax[1].set_title('Cor between simulated and fitted params')\n",
    "# ax[1].set_ylabel('Simulated parameters')\n",
    "ax[1].set_xlabel('Fitted parameters')\n",
    "ax[1].legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n",
    "# image export\n",
    "# plt.savefig('f4.pgf', format='pgf')\n",
    "\n",
    "# 3d plots\n",
    "trials_axis, participants_axis = np.meshgrid(trials_num, participants_num)\n",
    "participants_axis.resize(len(participants_num)*len(trials_num))\n",
    "trials_axis.resize(len(participants_num)*len(trials_num))\n",
    "theta_cor_sample_mean = theta_cor_sample\n",
    "theta_cor_sample_mean_alpha = \\\n",
    "theta_cor_sample_mean[:, :, 0].reshape(len(participants_num)*len(trials_num))\n",
    "theta_cor_sample_mean_beta = \\\n",
    "theta_cor_sample_mean[:, :, 1].reshape(len(participants_num)*len(trials_num))\n",
    "\n",
    "# 3d plot 1\n",
    "fig = plt.figure(figsize = (6,4.7))\n",
    "ax = fig.gca(projection = '3d')\n",
    "plot_g_1 = ax.plot_trisurf(participants_axis, trials_axis, theta_cor_sample_mean_alpha, \\\n",
    "                           cmap = plt.cm.viridis, linewidth = 0, vmin=-0.2, vmax=0.6)\n",
    "fig.colorbar(plot_g_1)\n",
    "ax.set_title(r'Pearson’s correlation between simulated and fitted $\\alpha$')\n",
    "ax.set_ylabel('Trials (1 to 5/10/15...)')\n",
    "ax.set_xlabel('Participants (1 to 2/4/6...)')\n",
    "plt.show()\n",
    "\n",
    "# image export\n",
    "# plt.savefig('s1.pgf', format='pgf')\n",
    "\n",
    "# 3d plot 2\n",
    "fig = plt.figure(figsize = (6,4.7))\n",
    "ax = fig.gca(projection = '3d')\n",
    "plot_g_2 = ax.plot_trisurf(participants_axis, trials_axis, theta_cor_sample_mean_beta, \\\n",
    "                           cmap = plt.cm.viridis, linewidth = 0, vmin=-0.2, vmax=0.6)\n",
    "fig.colorbar(plot_g_2)\n",
    "ax.set_title(r'Pearson’s correlation between simulated and fitted $\\beta$')\n",
    "ax.set_ylabel('Trials (1 to 5/10/15...)')\n",
    "ax.set_xlabel('Participants (1 to 2/4/6...)')\n",
    "plt.show()\n",
    "\n",
    "# image export\n",
    "# plt.savefig('s2.pgf', format='pgf')\n",
    "\n",
    "# output\n",
    "print(\"Pearson's correlation of alpha:\", [round(i,2) for i in theta_cor_samples[:, 0]])\n",
    "print(\"Pearson's correlation of beta:\", [round(i,2) for i in theta_cor_samples[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2653a7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-8f618d34b796>:21: RuntimeWarning: overflow encountered in exp\n",
      "  nll_A[i] = 1 / (1 + np.exp(-theta[1] * (V[c_alt-1] - V[c-1])))\n",
      "<ipython-input-10-8f618d34b796>:23: RuntimeWarning: divide by zero encountered in log\n",
      "  nll_A = -np.sum(np.log(nll_A))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of fitted parameter values [alpha, beta, A] by NLL(A): [8.29, 2.98, 8.88]\n",
      "Mean of fitted parameter values [alpha, beta, A] by restricted NLL(A): [0.4, 5.68, 0.98]\n",
      "Variance of fitted parameter values [alpha, beta, A] by NLL(A): [123.44, 10.89, 123.83]\n",
      "Variance of fitted parameter values [alpha, beta, A] by restricted NLL(A): [0.02, 4.66, 0.0]\n",
      "Pearson’s correlation coefficient (total) between alpha and A: 0.29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-8f618d34b796>:87: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# (h): Alternative model (9)\n",
    "\n",
    "# update strategy 2\n",
    "def update_2(V, action, outcome, alpha):\n",
    "    V[action-1] = alpha[0] * V[action-1] + alpha[1] * (outcome - V[action-1])\n",
    "    return V\n",
    "\n",
    "# simulation() remains the same as before by setting the config:\n",
    "# update = update_2\n",
    "# alpha = [alpha, A]\n",
    "\n",
    "# negative log likelihood\n",
    "def nll_A(theta, choices, outcomes, V0):\n",
    "    # theta = [alpha, beta, A]\n",
    "    length_num = len(choices)\n",
    "    V = copy.deepcopy(V0)\n",
    "    nll_A = np.zeros(length_num)\n",
    "    for i in range(length_num):\n",
    "        c = int(choices[i])\n",
    "        c_alt = 1 + int(c==1)\n",
    "        nll_A[i] = 1 / (1 + np.exp(-theta[1] * (V[c_alt-1] - V[c-1])))\n",
    "        V[c-1] = theta[2] * V[c-1] + theta[0] * (int(outcomes[i]) - V[c-1])\n",
    "    nll_A = -np.sum(np.log(nll_A))\n",
    "    return nll_A\n",
    "\n",
    "# optimization\n",
    "V0 = [0.5, 0.5]\n",
    "theta_opt_A = np.zeros([50,3])\n",
    "nll_A_obj = np.zeros(50)\n",
    "for i in range(50):    \n",
    "    nll_A_result = minimize(nll_A, [0.4, 5, 0.5], \\\n",
    "                            args = (choices.loc[i], outcomes.loc[i], V0), \\\n",
    "                            method = \"Nelder-Mead\")\n",
    "    theta_opt_A[i, :] = nll_A_result['x']\n",
    "    nll_A_obj[i] = nll_A_result['fun']\n",
    "\n",
    "# negative log likelihood\n",
    "def nll_A1(theta, choices, outcomes, V0):\n",
    "    # theta = [alpha, beta, np.log(np.divide(1, A) - 1)]\n",
    "    length_num = len(choices)\n",
    "    V = copy.deepcopy(V0)\n",
    "    nll_A1 = np.zeros(length_num)\n",
    "    for i in range(length_num):\n",
    "        c = int(choices[i])\n",
    "        c_alt = 1 + int(c==1)\n",
    "        nll_A1[i] = 1 / (1 + np.exp(-theta[1] * (V[c_alt-1] - V[c-1])))\n",
    "        V[c-1] = V[c-1] / (1 + np.exp(theta[2])) + theta[0] * \\\n",
    "        (int(outcomes[i]) - V[c-1])\n",
    "    nll_A1 = -np.sum(np.log(nll_A1))\n",
    "    return nll_A1\n",
    "\n",
    "# optimization\n",
    "V0 = [0.5, 0.5]\n",
    "theta_opt_A1 = np.zeros([50,3])\n",
    "for i in range(50):    \n",
    "    theta_opt_A1[i, :] = minimize(nll_A1, [0.4, 5, np.log(np.divide(1, 0.5) - 1)], \\\n",
    "                                  args = (choices.loc[i], outcomes.loc[i], V0), \\\n",
    "                                  method = \"Nelder-Mead\")['x']\n",
    "theta_opt_A1[:, 2] = 1 / (1 + np.exp(theta_opt_A1[:, 2]))\n",
    "\n",
    "# statistics\n",
    "theta_mean_A = np.mean(theta_opt_A, axis = 0)\n",
    "theta_var_A = np.var(theta_opt_A, axis = 0)\n",
    "theta_mean_A1 = np.mean(theta_opt_A1, axis = 0)\n",
    "theta_var_A1 = np.var(theta_opt_A1, axis = 0)\n",
    "theta_cor_A1 = np.corrcoef(theta_opt_A1[:, 0], theta_opt_A1[:, 2])[0, 1]\n",
    "\n",
    "# plot\n",
    "theta_opt_A1[:, 1] = 0.1 * theta_opt_A1[:, 1] # warning\n",
    "fig, ax =  plt.subplots(1, 2, figsize = (8,3.7))\n",
    "label_A = [r'$\\alpha$', r'$\\beta$', 'A']\n",
    "label_A1 = [r'$\\alpha$', r'0.1$\\beta$', 'A']\n",
    "color = ['#FF9671', '#FF6F91', '#D65DB1']\n",
    "# color = ['#FF6F91', '#D65DB1']\n",
    "for i in range(50):\n",
    "    ax[0].vlines(x = i+1, ymin = np.min(theta_opt_A[i, :]), ymax = \\\n",
    "                 np.max(theta_opt_A[i, :]), color = '#FFC75F', zorder = 1)\n",
    "    ax[1].vlines(x = i+1, ymin = np.min(theta_opt_A1[i, :]), ymax = \\\n",
    "                 np.max(theta_opt_A1[i, :]), color = '#FFC75F', zorder = 1)\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x = 25.5, ymin = 0, ymax = 1, linestyle = \"dashed\", \\\n",
    "                  color = '#FFC75F', zorder = 2)\n",
    "for i in range(3):\n",
    "    ax[0].scatter(np.arange(1, 51, 1), theta_opt_A[:, i], label = label_A[i], \\\n",
    "                  color = color[i], zorder = 3)\n",
    "    ax[1].scatter(np.arange(1, 51, 1), theta_opt_A1[:, i], label = label_A1[i], \\\n",
    "                  color = color[i], zorder = 3)\n",
    "ax[0].set_xlabel('Participant index')\n",
    "ax[1].set_xlabel('Participant index')\n",
    "ax[0].set_ylabel('Parameter value')\n",
    "ax[0].legend(loc = 'upper right')\n",
    "ax[1].legend(loc = 'upper right')\n",
    "ax[0].set_title('Unrestricted A')\n",
    "ax[1].set_title('Restricted A ($=1/(1+e^{A^{*}})$)')\n",
    "# plt.suptitle('Parameter optimization of NLL by Nelder-Mead')\n",
    "theta_opt_A1[:, 1] = 10 * theta_opt_A1[:, 1] # warning\n",
    "plt.show()\n",
    "\n",
    "# output\n",
    "print(\"Mean of fitted parameter values [alpha, beta, A] by NLL(A):\", \\\n",
    "      [round(i,2) for i in theta_mean_A])\n",
    "print(\"Mean of fitted parameter values [alpha, beta, A] by restricted NLL(A):\", \\\n",
    "      [round(i,2) for i in theta_mean_A1])\n",
    "print(\"Variance of fitted parameter values [alpha, beta, A] by NLL(A):\", \\\n",
    "      [round(i,2) for i in theta_var_A])\n",
    "print(\"Variance of fitted parameter values [alpha, beta, A] by restricted NLL(A):\", \\\n",
    "      [round(i,2) for i in theta_var_A1])\n",
    "print(\"Pearson’s correlation coefficient (total) between alpha and A:\", \\\n",
    "      round(theta_cor_A1,2))\n",
    "\n",
    "# image export\n",
    "# plt.savefig('f5.pgf', format='pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b26c4cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-da62cae4bf68>:72: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of fitted parameter values [alpha+, alpha-, beta] by NLL(aa): [0.26, 0.52, 11.49]\n",
      "Variance of fitted parameter values [alpha+, alpha-, beta] by NLL(aa): [0.03, 0.02, 188.12]\n",
      "Mean of fitted parameter values [alpha+, alpha-, beta] in group 1: 0.29 0.58 14.8\n",
      "Mean of fitted parameter values [alpha+, alpha-, beta] in group 2: 0.23 0.47 8.18\n",
      "The t-statistic of alpha+: 0.98\n",
      "The p-value of alpha+: 0.33\n",
      "The t-statistic of alpha-: 2.84\n",
      "The p-value of alpha-: 0.01\n",
      "The t-statistic of beta: 1.72\n",
      "The p-value of beta: 0.09\n"
     ]
    }
   ],
   "source": [
    "# (k): Discussion and extra model (15)\n",
    "\n",
    "# update strategy 3\n",
    "def update_3(V, action, outcome, alpha):\n",
    "    V[action-1] = V[action-1] + ((1 - outcome) * alpha[0] + outcome * alpha[1]) \\\n",
    "    * (outcome - V[action-1])\n",
    "    return V\n",
    "\n",
    "# simulation() remains the same as before by setting the config:\n",
    "# update = update_3\n",
    "# alpha = [alpha_pos, alpha_neg]\n",
    "\n",
    "# negative log likelihood\n",
    "def nll_aa(theta, choices, outcomes, V0):\n",
    "    # theta = [alpha_pos, alpha_neg, beta]\n",
    "    length_num = len(choices)\n",
    "    V = copy.deepcopy(V0)\n",
    "    nll_aa = np.zeros(length_num)\n",
    "    for i in range(length_num):\n",
    "        c = int(choices[i])\n",
    "        c_alt = 1 + int(c==1)\n",
    "        nll_aa[i] = 1 / (1 + np.exp(-theta[2] * (V[c_alt-1] - V[c-1])))\n",
    "        V[c-1] = V[c-1] + ((1 - int(outcomes[i])) * theta[0] + int(outcomes[i]) \\\n",
    "                           * theta[1]) * (int(outcomes[i]) - V[c-1])\n",
    "    nll_aa = -np.sum(np.log(nll_aa))\n",
    "    return nll_aa\n",
    "\n",
    "# optimization\n",
    "V0 = [0.5, 0.5]\n",
    "theta_opt_aa = np.zeros([50,3])\n",
    "nll_aa_obj = np.zeros(50)\n",
    "for i in range(50):    \n",
    "    nll_aa_result = minimize(nll_aa, [0.25, 0.35, 6], \\\n",
    "                             args = (choices.loc[i], outcomes.loc[i], V0), \\\n",
    "                             method = \"Nelder-Mead\")\n",
    "    theta_opt_aa[i, :] = nll_aa_result['x']\n",
    "    nll_aa_obj[i] = nll_aa_result['fun']\n",
    "\n",
    "# statistics\n",
    "theta_mean_aa = np.mean(theta_opt_aa, axis = 0)\n",
    "theta_var_aa = np.var(theta_opt_aa, axis = 0)\n",
    "a_pos_mean_1 = theta_opt_aa[0:25, 0]\n",
    "a_pos_mean_2 = theta_opt_aa[25:50, 0]\n",
    "a_neg_mean_1 = theta_opt_aa[0:25, 1]\n",
    "a_neg_mean_2 = theta_opt_aa[25:50, 1]\n",
    "beta_aa_mean_1 = theta_opt_aa[0:25, 2]\n",
    "beta_aa_mean_2 = theta_opt_aa[25:50, 2]\n",
    "t_a_pos = stats.ttest_ind(a_pos_mean_1, a_pos_mean_2)\n",
    "t_a_neg = stats.ttest_ind(a_neg_mean_1, a_neg_mean_2)\n",
    "t_beta_aa = stats.ttest_ind(beta_aa_mean_1, beta_aa_mean_2)\n",
    "\n",
    "# plot\n",
    "theta_opt_aa[:, 2] = 0.1 * theta_opt_aa[:, 2] # warning\n",
    "fig, ax =  plt.subplots(1, 2, figsize = (8,3.7))\n",
    "label_aa = [r'$\\alpha^+$', r'$\\alpha^-$', r'0.1$\\beta$']\n",
    "color = ['#FF9671', '#FF6F91', '#D65DB1']\n",
    "for i in range(50):\n",
    "    for j in range(2):\n",
    "        ax[j].vlines(x = i+1, ymin = np.min(theta_opt_aa[i, :]), \\\n",
    "                     ymax = np.max(theta_opt_aa[i, :]), color = '#FFC75F', zorder = 1)\n",
    "for i in range(3):\n",
    "    for j in range(2):\n",
    "        ax[j].scatter(np.arange(1, 51, 1), theta_opt_aa[:, i], \\\n",
    "                      label = label_aa[i], color = color[i], zorder = 3)\n",
    "        ax[j].axhline(y = np.mean(theta_opt_aa[:, i]), xmin = 0, xmax = 1, \\\n",
    "                      linestyle = \"dashed\", color = color[i], zorder = 2)\n",
    "        for k in range(2):\n",
    "            ax[j].axhline(y = np.mean(theta_opt_aa[(25*k):(25*(k+1)), i]), \\\n",
    "                          xmin = 0.5*k, xmax = 0.5*(k+1), color = color[i], zorder = 2)\n",
    "for i in range(2):\n",
    "    ax[i].axvline(x = 25.5, ymin = 0, ymax = 1, linestyle = \"dashed\", \\\n",
    "                  color = '#FFC75F', zorder = 2)\n",
    "    ax[i].set_xlabel('Participant index')\n",
    "    ax[i].set_ylabel('Parameter value')\n",
    "    ax[i].legend(loc = 'upper right')\n",
    "    ax[i].set_xlabel('Participant index')\n",
    "ax[0].set_title(r'Model with NLL($\\alpha^{\\pm}$)')\n",
    "ax[1].set_title('Specified range')\n",
    "ax[1].set_ylim(-0.1, 1.6)\n",
    "theta_opt_aa[:, 2] = 10 * theta_opt_aa[:, 2] # warning\n",
    "plt.show()\n",
    "\n",
    "# output\n",
    "print(\"Mean of fitted parameter values [alpha+, alpha-, beta] by NLL(aa):\", \\\n",
    "      [round(i, 2) for i in theta_mean_aa])\n",
    "print(\"Variance of fitted parameter values [alpha+, alpha-, beta] by NLL(aa):\", \\\n",
    "      [round(i, 2) for i in theta_var_aa])\n",
    "print(\"Mean of fitted parameter values [alpha+, alpha-, beta] in group 1:\", \\\n",
    "      round(np.mean(a_pos_mean_1), 2), round(np.mean(a_neg_mean_1), 2), \\\n",
    "      round(np.mean(beta_aa_mean_1), 2))\n",
    "print(\"Mean of fitted parameter values [alpha+, alpha-, beta] in group 2:\", \\\n",
    "      round(np.mean(a_pos_mean_2), 2), round(np.mean(a_neg_mean_2), 2), \\\n",
    "      round(np.mean(beta_aa_mean_2), 2))\n",
    "print(\"The t-statistic of alpha+:\", round(t_a_pos[0], 2))\n",
    "print(\"The p-value of alpha+:\", round(t_a_pos[1], 2))\n",
    "print(\"The t-statistic of alpha-:\", round(t_a_neg[0], 2))\n",
    "print(\"The p-value of alpha-:\", round(t_a_neg[1], 2))\n",
    "print(\"The t-statistic of beta:\", round(t_beta_aa[0], 2))\n",
    "print(\"The p-value of beta:\", round(t_beta_aa[1], 2))\n",
    "\n",
    "# image export\n",
    "# plt.savefig('f6.pgf', format='pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8f661ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIC of NLL(a): 6070.43 2601.82 3468.62\n",
      "AIC of NLL(A): 6123.36 2628.36 3495.01\n",
      "AIC of NLL(aa): 6043.09 2591.55 3451.54\n",
      "BIC of NLL(a): 6377.95 2601.82 3468.62\n",
      "BIC of NLL(A): 6584.64 2628.36 3495.01\n",
      "BIC of NLL(aa): 6504.37 2591.55 3451.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-d02f7444c640>:26: UserWarning: Matplotlib is currently using pgf, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "# (i): Model comparison (8)\n",
    "\n",
    "# plot\n",
    "fig, ax =  plt.subplots(1, 1, sharex = True, figsize = (7,3.7))\n",
    "label = ['Model 1', 'Model 2', 'Model 3']\n",
    "color = ['#FF9671', '#FF6F91', '#D65DB1']\n",
    "ax.scatter(np.arange(1, 51, 1), nll_a_obj, label = label[0], \\\n",
    "           color = color[0], zorder = 3)\n",
    "ax.scatter(np.arange(1, 51, 1), nll_A_obj, label = label[1], \\\n",
    "           color = color[1], zorder = 3)\n",
    "ax.scatter(np.arange(1, 51, 1), nll_aa_obj, label = label[2], \\\n",
    "           color = color[2], zorder = 3)\n",
    "for i in range(50):\n",
    "    plt.vlines(x = i+1, ymin = np.min([nll_a_obj[i], nll_A_obj[i], nll_aa_obj[i]]), \\\n",
    "               ymax = np.max([nll_a_obj[i], nll_A_obj[i], nll_aa_obj[i]]), \\\n",
    "               color = '#FFC75F', zorder = 1)\n",
    "plt.axvline(x = 25.5, ymin = 0, ymax = 1, linestyle = \"dashed\", \\\n",
    "            color = '#FFC75F', zorder = 2)\n",
    "plt.axhline(y = np.mean(nll_a_obj), xmin = 0, xmax = 1, \\\n",
    "            linestyle = \"dashed\", color = color[0], zorder = 2)\n",
    "plt.axhline(y = np.mean(nll_A_obj), xmin = 0, xmax = 1, \\\n",
    "            linestyle = \"dashed\", color = color[1], zorder = 2)\n",
    "plt.axhline(y = np.mean(nll_aa_obj), xmin = 0, xmax = 1, \\\n",
    "            linestyle = \"dashed\", color = color[2], zorder = 2)\n",
    "plt.axhline(y = np.mean(nll_a_obj[0:25]), xmin = 0, xmax = 0.5, \\\n",
    "            color = color[0], zorder = 2)\n",
    "plt.axhline(y = np.mean(nll_A_obj[0:25]), xmin = 0, xmax = 0.5, \\\n",
    "            color = color[1], zorder = 2)\n",
    "plt.axhline(y = np.mean(nll_aa_obj[0:25]), xmin = 0, xmax = 0.5, \\\n",
    "            color = color[2], zorder = 2)\n",
    "plt.axhline(y = np.mean(nll_a_obj[25:50]), xmin = 0.5, xmax = 1, \\\n",
    "            color = color[0], zorder = 2)\n",
    "plt.axhline(y = np.mean(nll_A_obj[25:50]), xmin = 0.5, xmax = 1, \\\n",
    "            color = color[1], zorder = 2)\n",
    "plt.axhline(y = np.mean(nll_a_obj[25:50]), xmin = 0.5, xmax = 1, \\\n",
    "            color = color[2], zorder = 2)\n",
    "ax.set_title('NLL values')\n",
    "ax.set_xlabel('Participant index')\n",
    "ax.set_ylabel('NLL objective function values')\n",
    "ax.legend(loc = 'upper right')\n",
    "plt.show()\n",
    "\n",
    "# AIC and BIC\n",
    "aic_nll_a = np.sum(2 * nll_a_obj + 2 * 2)\n",
    "aic_nll_A = np.sum(2 * nll_A_obj + 2 * 3)\n",
    "aic_nll_aa = np.sum(2 * nll_aa_obj + 2 * 3)\n",
    "aic_nll_a_1 = np.sum(2 * nll_a_obj[0:25] + 2 * 2)\n",
    "aic_nll_A_1 = np.sum(2 * nll_A_obj[0:25] + 2 * 3)\n",
    "aic_nll_aa_1 = np.sum(2 * nll_aa_obj[0:25] + 2 * 3)\n",
    "aic_nll_a_2 = np.sum(2 * nll_a_obj[25:50] + 2 * 2)\n",
    "aic_nll_A_2 = np.sum(2 * nll_A_obj[25:50] + 2 * 3)\n",
    "aic_nll_aa_2 = np.sum(2 * nll_aa_obj[25:50] + 2 * 3)\n",
    "bic_nll_a = np.sum(2 * nll_a_obj + 2 * np.log(160))\n",
    "bic_nll_A = np.sum(2 * nll_A_obj + 3 * np.log(160))\n",
    "bic_nll_aa = np.sum(2 * nll_aa_obj + 3 * np.log(160))\n",
    "bic_nll_a_1 = np.sum(2 * nll_a_obj[0:25] + 2 * np.log(160))\n",
    "bic_nll_A_1 = np.sum(2 * nll_A_obj[0:25] + 3 * np.log(160))\n",
    "bic_nll_aa_1 = np.sum(2 * nll_aa_obj[0:25] + 3 * np.log(160))\n",
    "bic_nll_a_2 = np.sum(2 * nll_a_obj[25:50] + 2 * np.log(160))\n",
    "bic_nll_A_2 = np.sum(2 * nll_A_obj[25:50] + 3 * np.log(160))\n",
    "bic_nll_aa_2 = np.sum(2 * nll_aa_obj[25:50] + 3 * np.log(160))\n",
    "\n",
    "# output\n",
    "print(\"AIC of NLL(a):\", round(aic_nll_a, 2), \\\n",
    "      round(aic_nll_a_1, 2), round(aic_nll_a_2, 2))\n",
    "print(\"AIC of NLL(A):\", round(aic_nll_A, 2), \\\n",
    "      round(aic_nll_A_1, 2), round(aic_nll_A_2, 2))\n",
    "print(\"AIC of NLL(aa):\", round(aic_nll_aa, 2), \\\n",
    "      round(aic_nll_aa_1, 2), round(aic_nll_aa_2, 2))\n",
    "print(\"BIC of NLL(a):\", round(bic_nll_a, 2), \\\n",
    "      round(aic_nll_a_1, 2), round(aic_nll_a_2, 2))\n",
    "print(\"BIC of NLL(A):\", round(bic_nll_A, 2), \\\n",
    "      round(aic_nll_A_1, 2), round(aic_nll_A_2, 2))\n",
    "print(\"BIC of NLL(aa):\", round(bic_nll_aa, 2), \\\n",
    "      round(aic_nll_aa_1, 2), round(aic_nll_aa_2, 2))\n",
    "\n",
    "# image export\n",
    "# plt.savefig('s3.pgf', format='pgf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c6293337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-e10583a7e47c>:21: RuntimeWarning: overflow encountered in exp\n",
      "  nll_A[i] = 1 / (1 + np.exp(-theta[1] * (V[c_alt-1] - V[c-1])))\n",
      "<ipython-input-10-e10583a7e47c>:23: RuntimeWarning: divide by zero encountered in log\n",
      "  nll_A = -np.sum(np.log(nll_A))\n",
      "<ipython-input-5-8af80401329a>:11: RuntimeWarning: overflow encountered in exp\n",
      "  nll[i] = 1 / (1 + np.exp(-theta[1] * (V[c_alt-1] - V[c-1])))\n",
      "<ipython-input-5-8af80401329a>:13: RuntimeWarning: divide by zero encountered in log\n",
      "  nll = -np.sum(np.log(nll))\n",
      "<ipython-input-3-27bd0dce688c>:10: RuntimeWarning: overflow encountered in exp\n",
      "  p_action_A = 1 / (1 + np.exp(-beta * (V[1] - V[0])))\n",
      "<ipython-input-10-e10583a7e47c>:22: RuntimeWarning: overflow encountered in double_scalars\n",
      "  V[c-1] = theta[2] * V[c-1] + theta[0] * (int(outcomes[i]) - V[c-1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix under AIC is:\n",
      "[[0.66 0.1  0.24]\n",
      " [0.51 0.2  0.29]\n",
      " [0.43 0.12 0.45]]\n",
      "Confusion matrix under BIC is:\n",
      "[[0.92 0.01 0.06]\n",
      " [0.64 0.16 0.2 ]\n",
      " [0.74 0.04 0.22]]\n",
      "The t-statistic of alpha+: 0.98\n",
      "The p-value of alpha+: 0.33\n",
      "The t-statistic of alpha-: 2.84\n",
      "The p-value of alpha-: 0.01\n",
      "The t-statistic of beta: 1.72\n",
      "The p-value of beta: 0.09\n",
      "Degrees of freedom: 47\n"
     ]
    }
   ],
   "source": [
    "# (j) Model recovery and confusion matrix (10)\n",
    "\n",
    "# config\n",
    "theta_mean_set = np.zeros([6, 3])\n",
    "theta_std_set = np.zeros([6, 3])\n",
    "range_025_975 = np.zeros([6, 3, 2])\n",
    "# 95% interval in normal distribution, excluding extreme samples\n",
    "\n",
    "for i in range(2):\n",
    "    theta_mean_set[i, 0:2] = np.mean(theta_opt[(25*i):(25*(i+1)), :], \\\n",
    "                                     axis = 0) # group i mean[alpha, beta]\n",
    "    theta_std_set[i, 0:2] = np.std(theta_opt[(25*i):(25*(i+1)), :], \\\n",
    "                                   axis = 0) # group i std[alpha, beta]\n",
    "    theta_mean_set[i+2, :] = np.mean(theta_opt_A[(25*i):(25*(i+1)), :], \\\n",
    "                                     axis = 0) # group i mean[alpha, beta, A]\n",
    "    theta_std_set[i+2, :] = np.std(theta_opt_A[(25*i):(25*(i+1)), :], \\\n",
    "                                   axis = 0) # group i std[alpha, beta, A]\n",
    "    theta_mean_set[i+4, :] = np.mean(theta_opt_aa[(25*i):(25*(i+1)), :], \\\n",
    "                                     axis = 0) # group i mean[alpha+, alpha-, beta]\n",
    "    theta_std_set[i+4, :] = np.std(theta_opt_aa[(25*i):(25*(i+1)), :], \\\n",
    "                                   axis = 0) # group i std[alpha+, alpha-, beta]\n",
    "for i in range(6):\n",
    "    range_025_975[i, :, 0] = theta_mean_set[i, :] - 1.96 * theta_std_set[i, :]\n",
    "    range_025_975[i, :, 1] = theta_mean_set[i, :] + 1.96 * theta_std_set[i, :]\n",
    "theta_alpha_index = [[0], [0, 2], [0, 1]]\n",
    "theta_beta_index = [[1], [1], [2]]\n",
    "param_num_3 = [2, 3, 3]\n",
    "param_num_6 = [2, 2, 3, 3, 3, 3]\n",
    "V0 = [0.5, 0.5]\n",
    "n = 1\n",
    "update_set = [update_1, update_2, update_3]\n",
    "decision = softmax_decision\n",
    "subsample_size = 250\n",
    "nll_set = [nll, nll_A, nll_aa]\n",
    "theta_init_set = [np.mean(theta_opt, axis = 0), np.mean(theta_opt_A, axis = 0), \\\n",
    "                  np.mean(theta_opt_aa, axis = 0)]\n",
    "\n",
    "# sample parameters\n",
    "theta_subsample = np.zeros([6, 3, subsample_size])\n",
    "for k in range(6):\n",
    "    for i in range(param_num_6[k]):\n",
    "        for j in range(subsample_size):\n",
    "            while theta_subsample[k, i, j] <= range_025_975[k, i, 0] or \\\n",
    "            theta_subsample[k, i, j] >= range_025_975[k, i, 1] or \\\n",
    "            theta_subsample[k, i, j] == 0:\n",
    "                theta_subsample[k, i, j] = np.random.normal(theta_mean_set[k, i], \\\n",
    "                                                            theta_std_set[k, i])\n",
    "\n",
    "# simulation and fit by optimizing 3 different NLLs and statistics\n",
    "nll_values_3 = np.zeros([3, 3, 2*subsample_size]) # [nll_true, nll_fit, num]\n",
    "for p in range(3):\n",
    "    for q in range(2):\n",
    "        for i in range(subsample_size):\n",
    "            choices_subsample, outcomes_subsample = \\\n",
    "            simulation(V0, n, update_set[p], decision, \\\n",
    "                       theta_subsample[2*p+q, theta_alpha_index[p], i], \\\n",
    "                       theta_subsample[2*p+q, theta_beta_index[p], i], sample = True)\n",
    "            for k in range(3):\n",
    "                nll_values_3[p, k, subsample_size*q+i] = \\\n",
    "                minimize(nll_set[k], theta_init_set[k], args = \\\n",
    "                         (choices_subsample, outcomes_subsample, V0), \\\n",
    "                         method = \"Nelder-Mead\")['fun']\n",
    "\n",
    "# AIC and BIC\n",
    "aic_nll_3 = np.zeros([3, 3, 2*subsample_size])\n",
    "bic_nll_3 = np.zeros([3, 3, 2*subsample_size])\n",
    "for i in range(3):\n",
    "    aic_nll_3[:, i, :] = 2 * nll_values_3[:, i, :] + 2 * param_num_3[i]\n",
    "    bic_nll_3[:, i, :] = 2 * nll_values_3[:, i, :] + param_num_3[i] * np.log(160)\n",
    "\n",
    "# confusion matrix\n",
    "aic_cm = np.zeros([3, 3])\n",
    "bic_cm = np.zeros([3, 3])\n",
    "for i in range(3):\n",
    "    for j in range(subsample_size):\n",
    "        aic_win_index = np.argmin(aic_nll_3[i, :, j])\n",
    "        aic_cm[i, aic_win_index] = aic_cm[i, aic_win_index] + 1\n",
    "        bic_win_index = np.argmin(bic_nll_3[i, :, j])\n",
    "        bic_cm[i, bic_win_index] = bic_cm[i, bic_win_index] + 1\n",
    "aic_cm = np.round(aic_cm / subsample_size, 2)\n",
    "bic_cm = np.round(bic_cm / subsample_size, 2)\n",
    "\n",
    "# t-test\n",
    "alpha_pos_mean_1 = theta_opt_aa[0:25, 0]\n",
    "alpha_pos_mean_2 = theta_opt_aa[25:50, 0]\n",
    "alpha_neg_mean_1 = theta_opt_aa[0:25, 1]\n",
    "alpha_neg_mean_2 = theta_opt_aa[25:50, 1]\n",
    "beta_mean_1 = theta_opt_aa[0:25, 2]\n",
    "beta_mean_2 = theta_opt_aa[25:50, 2]\n",
    "t_alpha_pos = stats.ttest_ind(alpha_pos_mean_1, alpha_pos_mean_2)\n",
    "t_alpha_neg = stats.ttest_ind(alpha_neg_mean_1, alpha_neg_mean_2)\n",
    "t_beta = stats.ttest_ind(beta_mean_1, beta_mean_2)\n",
    "\n",
    "# output\n",
    "print(\"Confusion matrix under AIC is:\")\n",
    "print(aic_cm)\n",
    "print(\"Confusion matrix under BIC is:\")\n",
    "print(bic_cm)\n",
    "print(\"The t-statistic of alpha+:\", round(t_alpha_pos[0], 2))\n",
    "print(\"The p-value of alpha+:\", round(t_alpha_pos[1], 2))\n",
    "print(\"The t-statistic of alpha-:\", round(t_alpha_neg[0], 2))\n",
    "print(\"The p-value of alpha-:\", round(t_alpha_neg[1], 2))\n",
    "print(\"The t-statistic of beta:\", round(t_beta[0], 2))\n",
    "print(\"The p-value of beta:\", round(t_beta[1], 2))\n",
    "print(\"Degrees of freedom:\", 47) # (n1+n2-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc3a121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
